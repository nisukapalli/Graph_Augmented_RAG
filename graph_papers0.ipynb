{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nikhilisukapalli/Downloads/Capstone/mag_papers_0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "os.chdir('/Users/nikhilisukapalli/Downloads/Capstone/mag_papers_0')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500000 papers so far\n",
      "Processed 1000000 papers so far\n",
      "Processed 1500000 papers so far\n",
      "Processed 2000000 papers so far\n",
      "Processed 2500000 papers so far\n",
      "Processed 3000000 papers so far\n",
      "Processed 3500000 papers so far\n",
      "Processed 4000000 papers so far\n",
      "Processed 4500000 papers so far\n",
      "Processed 5000000 papers so far\n"
     ]
    }
   ],
   "source": [
    "paper_count = 0\n",
    "edge_count = 0\n",
    "\n",
    "with open('mag_papers_0.txt', 'r') as papers0:\n",
    "    for line in papers0:\n",
    "        try:\n",
    "            paper = json.loads(line)\n",
    "\n",
    "            paper_id = paper.get(\"id\")\n",
    "            title = paper.get(\"title\", \"\")\n",
    "            year = paper.get(\"year\", None)\n",
    "            authors = [a.get(\"name\", \"\") for a in paper.get(\"authors\", [])]\n",
    "            fos = [f.get(\"name\", \"\") for f in paper.get(\"fos\", [])]\n",
    "            references = paper.get(\"references\", [])\n",
    "            abstract_data = paper.get(\"indexed_abstract\", {})\n",
    "\n",
    "            if \"InvertedIndex\" in abstract_data:\n",
    "                length = abstract_data[\"IndexLength\"]\n",
    "                index = abstract_data[\"InvertedIndex\"]\n",
    "                abstract_words = [None] * length\n",
    "                for word, positions in index.items():\n",
    "                    for pos in positions:\n",
    "                        abstract_words[pos] = word\n",
    "                abstract = \" \".join(w for w in abstract_words if w)\n",
    "            else:\n",
    "                abstract = \"\"\n",
    "\n",
    "            G.add_node(paper_id, \n",
    "                       title=title, \n",
    "                       year=year,\n",
    "                       authors=authors, \n",
    "                       fos=fos, \n",
    "                       abstract=abstract)\n",
    "\n",
    "            for ref_id in references:\n",
    "                G.add_edge(paper_id, ref_id, type=\"cites\")\n",
    "                edge_count += 1\n",
    "            \n",
    "            paper_count += 1\n",
    "            if paper_count % 500000 == 0:\n",
    "                print(f\"Processed {paper_count} papers so far\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing line: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers (nodes): 16624247\n",
      "Number of citations (edges):  21102653\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of papers (nodes): {G.number_of_nodes()}\")\n",
    "print(f\"Number of citations (edges): \", edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1789 citations — Unknown Title\n",
      "1648 citations — Unknown Title\n",
      "1402 citations — Unknown Title\n",
      "1272 citations — Unknown Title\n",
      "1187 citations — Unknown Title\n",
      "1164 citations — Unknown Title\n",
      "1161 citations — Unknown Title\n",
      "1144 citations — Unknown Title\n",
      "1091 citations — Unknown Title\n",
      "1065 citations — Unknown Title\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "top_cited = heapq.nlargest(10, G.in_degree(), key=lambda x: x[1])\n",
    "for paper_id, citations in top_cited:\n",
    "    print(f\"{citations} citations — {G.nodes[paper_id].get('title', 'Unknown Title')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning nodes: 100%|██████████| 15687111/15687111 [17:11<00:00, 15206.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13,293,426 invalid nodes\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "invalid_nodes = []\n",
    "for n, d in tqdm(G.nodes(data=True), total=G.number_of_nodes(), desc=\"Scanning nodes\"):\n",
    "    if not d.get(\"title\") or not d.get(\"abstract\"):\n",
    "        invalid_nodes.append(n)\n",
    "\n",
    "print(f\"Found {len(invalid_nodes):,} invalid nodes\")\n",
    "\n",
    "G.remove_nodes_from(invalid_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(G, \"mag_graph_cleaned.gpickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
